Maven
	<dependencies>
		<dependency>
			<groupId>org.encog</groupId>
			<artifactId>encog-core</artifactId>
			<version>3.3.0</version>
		</dependency>
	</dependencies>
	<build>
		<plugins>
			<plugin>
				<groupId>org.apache.maven.plugins</groupId>
				<artifactId>maven-compiler-plugin</artifactId>
				<version>3.6.0</version>
				<configuration>
					<source>1.8</source>
					<target>1.8</target>
				</configuration>
			</plugin>
		</plugins>
	</build>

ML Database
	http://kdd.ics.uci.edu/

Basics
	Structure
		input layer : input neurons, bias neuron
		hidden layer : hidden neurons, bias neuron, (context neurons in RNN)
		output layer : output neuron
	Activation function
		Sigmoid : 0 ~ +1
		Hyperbolic : -1 ~ +1
	Create
		BasicNetwork network = new BasicNetwork();
		network.addLayer(new BasicLayer(null, true, 2));
		...
		network.getStructure().finalizeStructure();
		network.reset();
	Train
		double XOR_INPUT[][] = {{0, 0}, {1, 0}, {0, 1}, {1, 1}};
		double XOR_IDEAL[][] = {{0}	, {1}	, {1}	, {0}};
		MLDataSet trainingSet = new BasicMLDataSet(XOR_INPUT, XOR_IDEAL);
		MLTrain train = new ResilientPropagation(network, trainingSet);
		while (true) {
			train.iteration();
			if (train.getError() < 0.001) break;
		}
		train.finishTraining();
	Use
		for (MLDataPair pair : trainingSet) {
			MLData output = network.compute(pair.getInput());
			...
		}

Creating neural network
	BasicNetwork
		BasicNetwork network = new BasicNetwork();
		network.addLayer(new BasicLayer(null, true, 2));
		...
		network.getStructure().finalizeStructure();
		network.reset();
	Pattern
		NeuralNetworkPattern pattern = new FeedForwardPattern();
		pattern.setActivationFunction(new ActivationTANH());
		pattern.setInputNeurons(3);
		pattern.addHiddenLayer(50);
		pattern.setOutputNeurons(1);
		BasicNetwork network = (BasicNetwork) pattern.generate();
		network.reset();

Normalization
	Number
		NormalizedField fuelStats = new NormalizedField(NormalizationAction.Normalize, "fuel", 200, 0, 0.9, -0.9);
		double num = 100;
		double n = fuelStats.normalize(num);
		double d = fuelStats.deNormalize(n);
		System.out.println(num + " "+ n + " "+ d);
	Array
		NormalizeArray norm = new NormalizeArray();
		norm.setNormalizedHigh(1);
		norm.setNormalizedLow(-1);
		double[] rawDataArray = {32, 22};
		double[] normalizedSunspots = norm.process(rawDataArray);
		System.out.println(Arrays.toString(rawDataArray) + " " + Arrays.toString(normalizedSunspots));
	File
		File sourceFile = new File("src/main/resources/source.data");
		File targetFile = new File("src/main/resources/target.data");
		EncogAnalyst analyst = new EncogAnalyst();
		AnalystWizard wizard = new AnalystWizard(analyst);
		wizard.wizard(sourceFile, true, AnalystFileFormat.DECPNT_COMMA);
		AnalystNormalizeCSV norm = new AnalystNormalizeCSV();
		norm.analyze(sourceFile, true, CSVFormat.ENGLISH, analyst);
		norm.setProduceOutputHeaders(true);
		norm.normalize(targetFile);
		analyst.save(new File("src/main/resources/stats.ega"));
		analyst.load(new File("src/main/resources/stats.ega"));
		
Persistence
	Detail
		Saving neural networks and data for later use
	Encog
		EncogDirectoryPersistence.saveObject(new File(), network);
		EncogDirectoryPersistence.loadObject(new File());
	JavaSerialization (internally)
		SerializeObject.save(new File(), network);
		SerializeObject.load(new File());

Optimization
	Training strategy
		Detail
			Training optimization
			Greedy : only update network status at iteration when error rate improves
			HybridStrategy : switch to secondary training method when primary fails 
			ResetStrategy : resets network and restart training when training fails
			SmartLearningRate, SmartMomentum : backprogation training with auto adjustment
			StopTrainingStrategy : stops training
		Code
			MLTrain trainMain;
			MLTrain trainSub;
			trainMain.addStrategy(new Greedy());
			trainMain.addStrategy(new HybridStrategy(trainSub));
			trainMain.addStrategy(new StopTrainingStrategy());
	Pruning
		Detail
			Neural Network structure optimization
			Create skeletal neural network with optimizing parameters for hidden layers
		Code
			NeuralNetworkPattern pattern = new FeedForwardPattern();
			pattern.setInputNeurons(...);
			pattern.setOutputNeurons(...);
			pattern.setActivationFunction(...);
			PruneIncremental prune = new PruneIncremental(...);
			prune.addHiddenLayer(...);
			...
			prune.process();
			BasicNetwork best = prune.getBestNetwork();
			
Supervised training - Propagation
	Detail
		Feedforward neural network
		Supervised training based on training sets, weights updated based on error(expected-actual)
		Activation functions with derivatives
	Types
		Backpropagation : learning rate, momentum
		Manhattan Update Rule : fixed weight update
		Quick propagation(QPROP) : learning rate
		Resilient propagation(RPROP) : dynamic learning rate, recommended
		Scaled Conjugate Gradient(SCG)
		Levenberg Marquardt(LMA) : hybrid
	Code
		BasicNetwork network;
		MLDataSet trainingSet;
		MLTrain train = new ResilientPropagation(network, trainingSet);

Supervised training - Score based
	Detail
		Feedforward neural network
		Supervised training based on scoring system
		Can be used on training set, but on the fly method is recommended
	Types
		Genetic Algorithm : natural selection with mutations
		Simulated Annealing : temperature cooling, can solve local minimum problem in backpropagation
	Scoring
		On the fly
			MyScore score = new MyScore();	//MyScore implements CalculateScore
		Training set
			CalculateScore score = new TrainingSetScore(trainingSet);
	Code
		BasicNetwork network;
		MLTrain train = new MLMethodGeneticAlgorithm(new MethodFactory() {
			@Override
			public MLMethod factor() {
				return network;
			}
		}, score, 500);
		MLTrain train = new NeuralSimulatedAnnealing(network, score, 10, 2, 100);

Recurrent neural network
	Detail
		Have context neurons allowing feedback
		Results in network with memory
		Training set's order is very important, temporal
	Type
		Elman(SRN)
			Context & hidden connected bi-directionally
			context(current)->hidden(current) connection with weight
			hidden(previous)->context(current) without weight
			1 context neuron - 1 hidden neuron?
		Jordan(SRN)
			Context & output connected bi-directionally
			context(current)->output(current) connection with weight
			output(previous)->context(current) without weight
			1 context neuron per 1 output neuron
			Works better for many outputs
	Code
		ElmanPattern pattern = new ElmanPattern();
		JordanPattern pattern = new JordanPattern();

Other neural network
	ART1 : Adaptive Resonance Theory
		Uses only bipolar(boolean) input
		Used at classification
		No separate training routine, learns as it is used
	NEAT : NeuroEvolution of Augmenting Topologies
		Network that rebuilds internal structure based on Genetic Algorithm

Predictive/Temporal Neural Network
	Overview
		Predict future based on present and past
		Feedforward neural network, Simple recurrent neural network
	Temporal
		Structure
			TemporalMLDataSet
				Comprised of data structured based on time
			TemporalDataDescription
				Description of one unit of data to be used
				RAW, PERCENT_CHANGE, DELTA_CHANGE 
			TemporalPoint
				Time representation of data, represents data at a point in time
		Code
			TemporalMLDataSet result = new TemporalMLDataSet(WINDOW_SIZE_PAST, WINDOW_SIZE_FUTURE);
			TemporalDataDescription desc = new TemporalDataDescription(TemporalDataDescription.Type.RAW, true, true);
			result.addDescription(desc);
			for (int year = TRAIN_START; year < TRAIN_END; year++) {
				TemporalPoint point = new TemporalPoint(1);
				point.setSequence(year);
				point.setData(0, normalizedSunspots[year]);
				result.getPoints().add(point);
			}
			result.generate();
	Temporal market data : temporal version of stock market data
		Structure
			MarketMLDataSet
			MarketDataDescription
			MarketDataType
				OPEN, HIGH, LOW, CLOSE, VOLUME, ADJUSTED_CLOSE
			MarketPoint
			TickerSymbol
		Code
			MarketLoader loader = new YahooFinanceLoader();
			MarketMLDataSet market = new MarketMLDataSet(loader, PAST_WINDOW, FUTURE_WINDOW);
			market.addDescription(new MarketDataDescription(
					TICKER,
					MarketDataType.ADJUSTED_CLOSE,
					true, true));
			Calendar begin = new GregorianCalendar();
			Calendar end = new GregorianCalendar();
			begin.add(...);
			end.add(...);
			market.load(begin.getTime(), end.getTime());
			market.generate();

		