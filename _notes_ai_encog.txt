Maven
	<dependencies>
		<dependency>
			<groupId>org.encog</groupId>
			<artifactId>encog-core</artifactId>
			<version>3.3.0</version>
		</dependency>
	</dependencies>
	<build>
		<plugins>
			<plugin>
				<groupId>org.apache.maven.plugins</groupId>
				<artifactId>maven-compiler-plugin</artifactId>
				<version>3.6.0</version>
				<configuration>
					<source>1.8</source>
					<target>1.8</target>
				</configuration>
			</plugin>
		</plugins>
	</build>

Neural Network structure
	input layer - hidden layer - output layer
	
Activation function
	Sigmoid : values 0 ~ +1
		f(x) = (1+e.pow(-x)).pow(-1)
		
	Hyperbolic : values -1 ~ +1
		f(x) = (e.pow(2x)-1)/(e.pow(2x)+1)
		
Neural network
	Create
		BasicNetwork network = new BasicNetwork();
		network.addLayer(new BasicLayer(null, true, 2));
		...
		network.getStructure().finalizeStructure();
		network.reset();
	Train
		double XOR_INPUT[][] = {{0, 0}, {1, 0}, {0, 1}, {1, 1}};
		double XOR_IDEAL[][] = {{0}	, {1}	, {1}	, {0}};
		MLDataSet trainingSet = new BasicMLDataSet(XOR_INPUT, XOR_IDEAL);
		MLTrain train = new ResilientPropagation(network, trainingSet);
		while (true) {
			train.iteration();
			if (train.getError() < 0.001) break;
		}
		train.finishTraining();
	Use
		for (MLDataPair pair : trainingSet) {
			MLData output = network.compute(pair.getInput());
			...
		}

ML Database
	http://kdd.ics.uci.edu/

Normalization
	Number
		NormalizedField fuelStats = new NormalizedField(NormalizationAction.Normalize, "fuel", 200, 0, 0.9, -0.9);
		double num = 100;
		double n = fuelStats.normalize(num);
		double d = fuelStats.deNormalize(n);
		System.out.println(num + " "+ n + " "+ d);
	Array
		NormalizeArray norm = new NormalizeArray();
		norm.setNormalizedHigh(1);
		norm.setNormalizedLow(-1);
		double[] rawDataArray = {32, 22};
		double[] normalizedSunspots = norm.process(rawDataArray);
		System.out.println(Arrays.toString(rawDataArray) + " " + Arrays.toString(normalizedSunspots));
	File
		File sourceFile = new File("src/main/resources/source.data");
		File targetFile = new File("src/main/resources/target.data");
		EncogAnalyst analyst = new EncogAnalyst();
		AnalystWizard wizard = new AnalystWizard(analyst);
		wizard.wizard(sourceFile, true, AnalystFileFormat.DECPNT_COMMA);
		AnalystNormalizeCSV norm = new AnalystNormalizeCSV();
		norm.analyze(sourceFile, true, CSVFormat.ENGLISH, analyst);
		norm.setProduceOutputHeaders(true);
		norm.normalize(targetFile);
		analyst.save(new File("src/main/resources/stats.ega"));
		analyst.load(new File("src/main/resources/stats.ega"));
		
Persistence
	Encog
		EncogDirectoryPersistence.saveObject(new File(), network);
		EncogDirectoryPersistence.loadObject(new File());
	JavaSerialization (internally)
		SerializeObject.save(new File(), network);
		SerializeObject.load(new File());
	
Supervised training - Propagation
	Detail
		Feedforward neural network
		Supervised training based on training sets, weights updated based on error(expected-actual)
		Activation functions with derivatives
	Types
		Backpropagation : learning rate, momentum
		Manhattan Update Rule : fixed weight update
		Quick propagation(QPROP) : learning rate
		Resilient propagation(RPROP) : dynamic learning rate, recommended
		Scaled Conjugate Gradient(SCG)
		Levenberg Marquardt(LMA) : hybrid
	Code
		BasicNetwork network;
		MLDataSet trainingSet;
		MLTrain train = new ResilientPropagation(network, trainingSet);

Supervised training - Score based
	Detail
		Feedforward neural network
		Supervised training based on scoring system
		Can be used on training set, but on the fly method is recommended
	Types
		Genetic Algorithm : natural selection with mutations
		Simulated Annealing : temperature cooling, can solve local minimum problem in backpropagation
	Scoring
		On the fly
			MyScore score = new MyScore();	//MyScore implements CalculateScore
		Training set
			CalculateScore score = new TrainingSetScore(trainingSet);
	Code
		BasicNetwork network;
		MLTrain train = new MLMethodGeneticAlgorithm(new MethodFactory() {
			@Override
			public MLMethod factor() {
				return network;
			}
		}, score, 500);
		MLTrain train = new NeuralSimulatedAnnealing(network, score, 10, 2, 100);
		
Creating neural network
	BasicNetwork
		BasicNetwork network = new BasicNetwork();
		network.addLayer(new BasicLayer(null, true, 2));
		...
		network.getStructure().finalizeStructure();
		network.reset();
	Pattern
		FeedForwardPattern pattern = new FeedForwardPattern();
		pattern.setActivationFunction(new ActivationTANH());
		pattern.setInputNeurons(3);
		pattern.addHiddenLayer(50);
		pattern.setOutputNeurons(1);
		BasicNetwork network = (BasicNetwork) pattern.generate();
		network.reset();

Recurrent neural network
	Detail
		Have context neurons allowing feedback
		Results in network with memory
		Training set's order is very important, temporal
	Type
		Elman(SRN)
			Context & hidden connected bi-directionally
			context(current)->hidden(current) connection with weight
			hidden(previous)->context(current) without weight
			1 context neuron - 1 hidden neuron?
		Jordan(SRN)
			Context & output connected bi-directionally
			context(current)->output(current) connection with weight
			output(previous)->context(current) without weight
			1 context neuron per 1 output neuron
			Works better for many outputs
	Code
		ElmanPattern pattern = new ElmanPattern();
		JordanPattern pattern = new JordanPattern();

Other neural network
	ART1 : Adaptive Resonance Theory
		Uses only bipolar(boolean) input
		Used at classification
		No separate training routine, learns as it is used
	NEAT : NeuroEvolution of Augmenting Topologies
		Network that rebuilds internal structure based on Genetic Algorithm

Training strategy
	Type
		Hybrid : switch training methods
		Greedy : only select improving training (update weight?)
		Stop : stop training
	Code
		MLTrain trainMain;
		MLTrain trainSub;
		trainMain.addStrategy(new Greedy());
		trainMain.addStrategy(new HybridStrategy(trainSub));
		trainMain.addStrategy(new StopTrainingStrategy());